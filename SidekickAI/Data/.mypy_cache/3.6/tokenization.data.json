{".class": "MypyFile", "_fullname": "tokenization", "is_partial_stub_package": false, "is_stub": false, "names": {".class": "SymbolTable", "__doc__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "tokenization.__doc__", "name": "__doc__", "type": "builtins.str"}}, "__file__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "tokenization.__file__", "name": "__file__", "type": "builtins.str"}}, "__name__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "tokenization.__name__", "name": "__name__", "type": "builtins.str"}}, "__package__": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": ["is_ready"], "fullname": "tokenization.__package__", "name": "__package__", "type": "builtins.str"}}, "base_tokenize": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [0, 1], "arg_names": ["sentence", "full_token"], "flags": [], "fullname": "tokenization.base_tokenize", "name": "base_tokenize", "type": null}}, "check_huggingface": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [], "arg_names": [], "flags": [], "fullname": "tokenization.check_huggingface", "name": "check_huggingface", "type": null}}, "check_spacy": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [], "arg_names": [], "flags": [], "fullname": "tokenization.check_spacy", "name": "check_spacy", "type": null}}, "load_bert_wordpiece": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [1, 1], "arg_names": ["lowercase", "overwrite"], "flags": [], "fullname": "tokenization.load_bert_wordpiece", "name": "load_bert_wordpiece", "type": null}}, "load_sentencepiece": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [1, 1], "arg_names": ["lowercase", "overwrite"], "flags": [], "fullname": "tokenization.load_sentencepiece", "name": "load_sentencepiece", "type": null}}, "load_spacy_large": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [1], "arg_names": ["overwrite"], "flags": [], "fullname": "tokenization.load_spacy_large", "name": "load_spacy_large", "type": null}}, "load_spacy_med": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [1], "arg_names": ["overwrite"], "flags": [], "fullname": "tokenization.load_spacy_med", "name": "load_spacy_med", "type": null}}, "load_spacy_small": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [1], "arg_names": ["overwrite"], "flags": [], "fullname": "tokenization.load_spacy_small", "name": "load_spacy_small", "type": null}}, "spacyNLP": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": [], "fullname": "tokenization.spacyNLP", "name": "spacyNLP", "type": {".class": "UnionType", "items": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "NoneType"}]}}}, "tokenize": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [0, 1], "arg_names": ["sentence", "full_token"], "flags": [], "fullname": "tokenization.tokenize", "name": "tokenize", "type": null}}, "tokenizer": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "Var", "flags": [], "fullname": "tokenization.tokenizer", "name": "tokenizer", "type": {".class": "UnionType", "items": [{".class": "AnyType", "missing_import_name": null, "source_any": null, "type_of_any": 1}, {".class": "NoneType"}]}}}, "traverseList": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [0], "arg_names": ["sentence"], "flags": [], "fullname": "tokenization.traverseList", "name": "traverseList", "type": null}}, "untokenize_wordpiece": {".class": "SymbolTableNode", "kind": "Gdef", "node": {".class": "FuncDef", "arg_kinds": [0, 1], "arg_names": ["tokens", "EOS_token"], "flags": [], "fullname": "tokenization.untokenize_wordpiece", "name": "untokenize_wordpiece", "type": null}}}, "path": "/Users/jafioti/Documents/Sidekick/SidekickAI/Data/tokenization.py"}